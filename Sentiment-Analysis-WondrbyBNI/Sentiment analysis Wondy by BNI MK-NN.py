# -*- coding: utf-8 -*-
"""Clean MKNN Prompt for 4k data .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SlSvLsMYDlwfUx2pyGANLoSLDv0AqOW6
"""

#import library yang digunakan
import pandas as pd
import numpy as np
from datetime import date
import re, string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
from nltk.stem import SnowballStemmer
from nltk.corpus import wordnet
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('punkt_tab')

#import data
from google.colab import files
uploaded = files.upload()

#menampilkan data yang telah di import
df = pd.read_csv('scrapped_data.csv')
df.head()

"""pre-processing data"""

#1. cleaning data untuk menghapus emoji, url, tagar, tanda baca, dan spasi berulang

import re
def clean_text(text):
    # 1. Remove emojis
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U00002702-\U000027B0"  # other symbols
                           u"\U000024C2-\U0001F251"  # enclosed characters
                           "]+", flags=re.UNICODE)
    text = emoji_pattern.sub(r'', text)

    # 2. Remove hashtags (words starting with #)
    text = re.sub(r'#\w+', '', text)

    # 3. Remove URLs (http/https links)
    text = re.sub(r'http\S+|www\S+', '', text)

    #4. remove punctuation (. , ,, ?, !)
    text = text.translate(str.maketrans('', '', string.punctuation))

    #5. remove extra space
    text = text.strip()

    return text

df['cleaned_text'] = df['content'].apply(clean_text)
df.head()

#2. mengubah cleaned_text menjadi lower case/ case folding

df['cleaned_text'] = df['cleaned_text'].str.lower()
df.head()

#3. melakukan tokenizing

df['tokenized_text'] = df['cleaned_text'].apply(word_tokenize)
df.head()

#4. stopword removal bahasa indonesia

indonesian_stopwords = stopwords.words('indonesian')

def remove_stopwords(tokens):
    filtered_tokens = [token for token in tokens if token not in indonesian_stopwords]
    return filtered_tokens

df['no_stopword_text'] = df['tokenized_text'].apply(remove_stopwords)
df.head()

#5. stemming

!pip install Sastrawi
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

factory = StemmerFactory()
stemmer = factory.create_stemmer()

def stem_text(tokens):
    stemmed_tokens = [stemmer.stem(token) for token in tokens]
    return stemmed_tokens

df['stemmed_text'] = df['no_stopword_text'].apply(stem_text)
df['stemmed_text'] = df['stemmed_text'].apply(' '.join)
df['tokenized_text']= df['stemmed_text'].apply(word_tokenize)
df['tokenized_text'].to_csv('preprocessingdata.csv', index=False)
df.head()

# prompt: simpan hasil stemming dalam format excel

# Assuming 'df' is your DataFrame with the 'stemmed_text' column

# Export the 'stemmed_text' column to an Excel file
df['stemmed_text'].to_excel('stemmed_data.xlsx', index=False)

# Download the Excel file (optional)
#from google.colab import files
#files.download('stemmed_data.xlsx')

# menghapus data kosong pada tokenized_text

df = df[df['tokenized_text'].apply(lambda x: isinstance(x, list) and len(x) > 0)]
df.head()

# menghitung berapa banyak data yang tersisa

remaining_data_count = len(df)
print(f"Jumlah data yang tersisa: {remaining_data_count}")

"""Pelabelan data"""

#labeling
def label_sentimen(score):
  if score <= 3:
    return 'Negatif'
  else:
    return 'Positif'

df.loc[:, 'sentimen'] = df['score'].apply(label_sentimen)
df

# Menghitung jumlah sentimen negatif dan positif
sentimen_counts = df['sentimen'].value_counts()

# Menampilkan hasil
print(sentimen_counts)

# menyajikan hasil pelabelan dengan bar chart

import matplotlib.pyplot as plt

# Menghitung jumlah sentimen positif dan negatif
sentimen_counts = df['sentimen'].value_counts()

# Membuat bar chart
plt.figure(figsize=(8, 6))
plt.bar(sentimen_counts.index, sentimen_counts.values, color=['blue', 'red'])
plt.title('Jumlah Sentimen Positif dan Negatif')
plt.xlabel('Sentimen')
plt.ylabel('Jumlah')
plt.show()

"""wordcloud"""

#Wordcloud untuk sentimen positif

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Filtering data untuk mengambil sentimen positif
positive_df = df[df['sentimen'] == 'Positif']

# Gabungkan semua stemmed_text untuk sentimen positif ke dalam satu string
text_positive = ' '.join(positive_df['stemmed_text'].tolist())

# Membentuk wordcloud
wordcloud_positive = WordCloud(width=800, height=400, background_color='white').generate(text_positive)

# Menyajikan word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_positive, interpolation='bilinear')
plt.axis('off')
plt.title('Wordcloud sentimen positif')
plt.show()

# wordcloud untuk sentimen negatif

import matplotlib.pyplot as plt
# Filtering data untuk mengambil sentimen negatif
negative_df = df[df['sentimen'] == 'Negatif']

# Gabungkan semua stemmed_text untuk sentimen positif ke dalam satu string
text_negative = ' '.join(negative_df['stemmed_text'].tolist())

# Membentuk wordcloud
wordcloud_negative = WordCloud(width=800, height=400, background_color='white').generate(text_negative)

# Menyajikan wordcloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_negative, interpolation='bilinear')
plt.axis('off')
plt.title('Wordcloud sentimen negatif')
plt.show()

# coding sentimen positif = 1 dan negatif = 0

def encode_sentimen(sentimen):
  if sentimen == 'Positif':
    return 1
  else:
    return 0

df['sentimen_encoded'] = df['sentimen'].apply(encode_sentimen)
df

"""menghitung pembobotan kata dengan TF-IDF dan membagi data traning dan data testing"""

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

# Perhitungan TF-IDF
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df['stemmed_text'])  # X adalah matriks TF-IDF
y = df['sentimen_encoded']  # y adalah label sentimen

# Membagi data menjadi data training dan testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train

X_test

y_train

y_test

"""menghitung nilai cosine similarity"""

import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity

# Menghitung cosine similarity antara semua dokumen dalam data latih (TF-IDF matrix atau data X_train)
cosine_sim_matrix = cosine_similarity(X_train)

# Menampilkan matriks cosine similarity
print("Matriks Cosine Similarity:")
print(cosine_sim_matrix)

# Konversi matriks cosine similarity ke DataFrame untuk menyimpan ke file CSV
cosine_sim_df = pd.DataFrame(cosine_sim_matrix)

# Menyimpan hasil cosine similarity ke dalam file CSV
cosine_sim_df.to_csv('cosine_similarity_matrix.csv', index=False)

print("Matriks Cosine Similarity telah disimpan ke file 'cosine_similarity_matrix.csv'.")

"""mengurutkan nilai cosine similarity"""

import numpy as np
import pandas as pd

# Mengurutkan nilai cosine similarity dari terkecil hingga terbesar
sorted_cosine_sim = np.sort(cosine_sim_matrix, axis=None)

# Menampilkan nilai cosine similarity yang telah diurutkan
print("Nilai Cosine Similarity yang Diurutkan:")
print(sorted_cosine_sim)

# Konversi array yang sudah diurutkan menjadi DataFrame
sorted_cosine_sim_df = pd.DataFrame(sorted_cosine_sim, columns=['Cosine Similarity'])

# Menyimpan hasil urutan cosine similarity ke dalam file CSV
sorted_cosine_sim_df.to_csv('sorted_cosine_similarity.csv', index=False)

print("Nilai Cosine Similarity yang telah diurutkan telah disimpan ke file 'sorted_cosine_similarity.csv'.")

"""menghitung nilai validitas"""

# menghitung nilai validitas

import numpy as np
def calculate_validity(cosine_sim_matrix, y_train):

  K=2     #memasukkan nilai K yang diinginkan
  validity_values = []
  for i in range(cosine_sim_matrix.shape[0]):
    neighbor_indices = np.argsort(cosine_sim_matrix[i])[::-1][1:3]

    validity = 0
    for neighbor_index in neighbor_indices:
      if y_train.iloc[neighbor_index] == y_train.iloc[i]:
        validity += 1

    validity_values.append((1/K) * validity)

  return validity_values

# Menghitung nilai Validitas untuk data latih
validity_values = calculate_validity(cosine_sim_matrix, y_train)

# Menyajikan nilai Validitas untul 10 data pertama
print("Validity Values (first 10 data points):", validity_values[:10])

"""menghitung jarak data latih dan data uji"""

#Menghitung jarak dokumen data latih dan data uji menggunakan cosine distance

import pandas as pd
from sklearn.metrics.pairwise import cosine_distances

# Menghitung cosine distance antara data latih (X_train) dan data uji (X_test)
cosine_dist_matrix = cosine_distances(X_train, X_test)

# Menampilkan matriks cosine distance
print("Matriks Cosine Distance:")
print(cosine_dist_matrix)

# Konversi matriks cosine distance ke DataFrame untuk menyimpan ke file CSV
cosine_dist_df = pd.DataFrame(cosine_dist_matrix)

# Menyimpan hasil cosine distance ke dalam file CSV
cosine_dist_df.to_csv('cosine_distance_matrix.csv', index=False)

print("Matriks Cosine Distance telah disimpan ke file 'cosine_distance_matrix.csv'.")

"""menghitung nilai weight voting"""

# menghitung nilai weight voting antara data latih dan data uji

import pandas as pd
import numpy as np
weight_voting_matrix = np.zeros((X_train.shape[0], X_test.shape[0]))
validitas_df = pd.DataFrame({'validitas': validity_values}) # Create validitas_df from validity_values

for j in range(X_test.shape[0]):
  for i in range(X_train.shape[0]):
    weight_voting_matrix[i, j] = validitas_df['validitas'].iloc[i] * (1 / (cosine_dist_matrix[i, j] + 0.5))

print("Matriks Weight Voting:")
print(weight_voting_matrix)

# Simpan matriks weight voting ke dalam file CSV
weight_voting_df = pd.DataFrame(weight_voting_matrix)
weight_voting_df.to_csv('weight_voting_matrix.csv', index=False)

print("Matriks Weight Voting telah disimpan ke file 'weight_voting_matrix.csv'.")

# Check the shapes of the dataframes to investigate the potential issue.
print(f'Shape of validitas_df: {validitas_df.shape}')
print(f'Shape of X_train: {X_train.shape}')

"""menentukan kelas data uji"""

# menentukan sentimen dari setiap data uji menggunakan nilai weight voting terbesar

import pandas as pd
import numpy as np

predicted_sentiments = []

for j in range(X_test.shape[0]):
  current_test_weights = weight_voting_matrix[:, j]
  max_weight_index = np.argmax(current_test_weights)
  predicted_sentiment = y_train.iloc[max_weight_index]
  predicted_sentiments.append(predicted_sentiment)

# Konversi list hasil prediksi sentimen ke dalam DataFrame
predicted_sentiments_df = pd.DataFrame({'predicted_sentiment': predicted_sentiments})

# Simpan hasil prediksi sentimen ke dalam file CSV
predicted_sentiments_df.to_csv('predicted_sentiments.csv', index=False)

print("Hasil prediksi sentimen telah disimpan ke file 'predicted_sentiments.csv'.")
print (predicted_sentiments_df)

"""Evaluasi Model"""

# confusion matrix

from sklearn.metrics import confusion_matrix

# Assuming y_test contains the true labels and predicted_sentiments contains the predicted labels
cm = confusion_matrix(y_test, predicted_sentiments)

print("Confusion Matrix:")
print(cm)

# You can also visualize the confusion matrix using seaborn if you want
import seaborn as sns
import matplotlib.pyplot as plt

sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

# precission, recall, and accuracy

from sklearn.metrics import precision_score, recall_score, accuracy_score

# Calculate precision
precision = precision_score(y_test, predicted_sentiments)

# Calculate recall
recall = recall_score(y_test, predicted_sentiments)

# Calculate accuracy
accuracy = accuracy_score(y_test, predicted_sentiments)

# Print the results
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"Accuracy: {accuracy}")